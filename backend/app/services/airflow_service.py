"""
Airflow DAG generation and management service.

This service handles the creation, update, and deletion of Airflow DAG files
for scheduled ETL jobs.
"""
import os
from typing import Optional
from pathlib import Path

from app.models.etl_job import ETLJob
from app.models.schedule import Schedule
from app.core.config import settings


class AirflowService:
    """Service for managing Airflow DAG files."""

    def __init__(self, dags_dir: Optional[str] = None):
        """
        Initialize the Airflow service.

        Args:
            dags_dir: Directory where DAG files are stored. Defaults to settings.AIRFLOW_DAGS_DIR
        """
        self.dags_dir = dags_dir or settings.AIRFLOW_DAGS_DIR
        # Ensure the directory exists
        Path(self.dags_dir).mkdir(parents=True, exist_ok=True)

    def generate_scheduled_dag(self, job: ETLJob, schedule: Schedule) -> str:
        """
        Generate an Airflow DAG file for a scheduled ETL job.

        This creates a Python file that defines a DAG which triggers the ETL job
        execution endpoint on a schedule.

        Args:
            job: The ETL job to schedule
            schedule: The schedule configuration

        Returns:
            The DAG ID that was generated
        """
        dag_id = schedule.airflow_dag_id or f"etl_job_{job.id}_scheduled"

        # Get API URL from environment
        api_url = os.getenv("ETL_PORTAL_API_URL", "http://backend:8000")

        dag_code = f'''"""
Auto-generated Airflow DAG for ETL Job #{job.id}: {job.name}
Generated by ETL Portal Schedule Manager
"""
from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta

default_args = {{
    'owner': 'etl_portal',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}}

# DAG definition
dag = DAG(
    dag_id='{dag_id}',
    default_args=default_args,
    description='Scheduled execution for ETL job: {job.name}',
    schedule_interval='{schedule.cron_expression}',
    catchup=False,
    tags=['etl-portal', 'scheduled', 'job-{job.id}'],
    max_active_runs=1,
)

# Task: Trigger ETL job execution via API
execute_job = SimpleHttpOperator(
    task_id='execute_etl_job_{job.id}',
    http_conn_id='etl_portal_api',
    endpoint='/api/v1/jobs/execute/{job.id}',
    method='POST',
    headers={{'Content-Type': 'application/json'}},
    response_check=lambda response: response.status_code == 201,
    dag=dag,
)
'''

        # Write DAG file
        dag_path = os.path.join(self.dags_dir, f"{dag_id}.py")
        with open(dag_path, 'w') as f:
            f.write(dag_code)

        return dag_id

    def update_scheduled_dag(self, job: ETLJob, schedule: Schedule) -> str:
        """
        Update an existing Airflow DAG file.

        This is essentially the same as creating a new DAG - Airflow will
        automatically detect the changes and reload the DAG.

        Args:
            job: The ETL job
            schedule: The updated schedule configuration

        Returns:
            The DAG ID
        """
        return self.generate_scheduled_dag(job, schedule)

    def delete_dag(self, dag_id: str) -> bool:
        """
        Delete an Airflow DAG file.

        Args:
            dag_id: The DAG ID to delete

        Returns:
            True if the DAG was deleted, False if it didn't exist
        """
        dag_path = os.path.join(self.dags_dir, f"{dag_id}.py")

        if os.path.exists(dag_path):
            os.remove(dag_path)
            return True

        return False

    def dag_file_exists(self, dag_id: str) -> bool:
        """
        Check if a DAG file exists.

        Args:
            dag_id: The DAG ID to check

        Returns:
            True if the DAG file exists
        """
        dag_path = os.path.join(self.dags_dir, f"{dag_id}.py")
        return os.path.exists(dag_path)

    def get_dag_path(self, dag_id: str) -> str:
        """
        Get the full path to a DAG file.

        Args:
            dag_id: The DAG ID

        Returns:
            Full path to the DAG file
        """
        return os.path.join(self.dags_dir, f"{dag_id}.py")
