# Application Database Configuration (ETL Portal metadata)
POSTGRES_USER=etl_user
POSTGRES_PASSWORD=etl_password
POSTGRES_DB=etl_portal

# Test Database Configuration (ETL job destination)
TEST_DB_HOST=test-db
TEST_DB_PORT=5432
TEST_DB_USER=test_user
TEST_DB_PASSWORD=test_password
TEST_DB_NAME=test_db

# Backend Configuration
DATABASE_URL=postgresql+asyncpg://etl_user:etl_password@postgres:5432/etl_portal
REDIS_URL=redis://redis:6379/1

# Security - IMPORTANT: Generate new keys for production!
# Generate ENCRYPTION_KEY with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
ENCRYPTION_KEY=

# Generate SECRET_KEY with: openssl rand -hex 32
SECRET_KEY=your-secret-key-change-in-production

# Airflow Configuration
# Generate AIRFLOW_FERNET_KEY with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=

# File Upload
UPLOAD_DIR=/app/uploads
MAX_UPLOAD_SIZE=1000000000
TEMP_FILE_CLEANUP_HOURS=24

# ETL Processing
DEFAULT_BATCH_SIZE=10000
MAX_PREVIEW_ROWS=100

# CORS Origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Airflow
AIRFLOW_DAGS_DIR=./airflow/dags
AIRFLOW_API_URL=http://airflow-webserver:8080/api/v1

# Environment
ENVIRONMENT=development
DEBUG=True

# Frontend
VITE_API_URL=http://localhost:8000
